# Content Discovery

- Find out about files and directories on the server

- `/robots.txt` (see <http://www.robotstxt.org/robotstxt.html>)
- `/sitemap.xml`
- `/.well-known/`
- `/README.txt`
- `/.git`


seclists/Discovery/Web-Content









## nuclei

```

nuclei -u https://example.com -s critical,high,medium,low,info

```







## dirsearch

```

dirsearch --crawl -u $ip

dirsearch -u http://mailing.htb/ -x 403,404,400


dirsearch -e php -u http://pocsecurity.icu

dirsearch.py -e php -u https://example.com --exclude-status 403,401

dirsearch.py -l target.txt --deep-recursive

dirsearch --tor --crawl -u https://example.com --exclude-status 404 --deep-recursive --max-rate=5 -q --format=xml

```



## dirb / dirbuster / gobuster

`-x`: specify extention. Example: `-x php,html,txt,bak`, `-x php,py,html`


```
gobuster dir --url [URL] --wordlist [WORDLIST FILE]
gobuster dir -q --url [URL] --wordlist [WORDLIST FILE]

gobuster dir -u http://$IP -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt -x php,sh,txt,cgi,html,js,css,py





```

## nikto
```
nikto -h http://$IP | tee nikto.log
nikto -host $IP -output out.txt -port 80

nikto -h example.com -ssl

```




## ffuf
- <https://github.com/ffuf/ffuf>
- <https://medium.com/@qaafqasim/mastering-ffuf-basic-and-advanced-commands-60e53bdbffc7>

`-fc` filter status code



```

ffuf -w <wordlist> -u http://board.htb/ -H "Host: FUZZ.board.htb" -ac
ffuf -w <wordlist> -u http://board.htb/ -H "User-Agent: [USER AGENT]" -ac

ffuf -w wordlist_location -u www.google.com/FUZZ -H "User-Agent: your_user_agent" -p '0.5-10' -t 100


ffuf -w /usr/share/seclists/Discovery/DNS/subdomains-top1million-20000.txt -H "Host: FUZZ.example.com" -u http://[IP] -fc 302

ffuf -u http://[IP]/FUZZ -w /usr/share/seclists/Discovery/Web-Content/directory-list-2.3-medium.txt -e .txt,.php,.html,.htm


# recursion for in-depth subdirectory analysis
ffuf -w wordlist_location -u http://192.168.1.1/FUZZ -fc 301 --recursion --recursion-depth 2





```




## wfuzz
web application bruteforcer

- <https://wfuzz.readthedocs.io/en/latest/index.html>
- <https://otterhacker.github.io/Pentest/Tools/Wfuzz.html>


```
wfuzz -c -L --hh=1674 ???


# 1 Subdomain enumaration
wfuzz -c --hc 400,404 -t 200 --hl 9 -w subdomains-top1million-110000.txt -u http://permx.htb -H "Host: FUZZ.permx.htb"

# 2 Perform directory fuzzing on the discovered subdomain to find additional hidden paths.
wfuzz -c --hc 400,404 -t 200  -w subdomains-top1million-110000.txt -u http://lms.permx.htb/FUZZ


```



## nmap
```
sudo nmap -p80 --script=http-enum $ip

```


## masscan

```
amass enum -brute -active -d example.com -o amass-output.txt
amass enum -active -d example.com | httpx -sc -td -title -silent -o httpx.txt
cat httpx.txt | sed 's/http:\/\/\|https:\/\/\|ftp:\/\///g'
```

## subfinder
```
subfinder -silent -all -d example.com | httpx -silent -sc
subfinder -silent -all -d example.com | httpx | nuclei -s critical,high,medium,low,info

```









## Sublist3r
Passive


- <https://github.com/aboul3la/Sublist3r>

```
./sublist3r.py -d example.com | httpx -silent -sc

sublist3r -d hackersploit.org -e google,yahoo
```







## The Harvester
Can be used to subdomain enumeration.

- <https://github.com/laramies/theHarvester>





























## Subdomain enumeration

## SSL/TLS Certificates
When an SSL/TLS (Secure Sockets Layer/Transport Layer Security) certificate is created for a domain by a CA (Certificate Authority), CA's take part in what's called "Certificate Transparency (CT) logs". These are publicly accessible logs of every SSL/TLS certificate created for a domain name. The purpose of Certificate Transparency logs is to stop malicious and accidentally made certificates from being used. We can use this service to our advantage to discover subdomains belonging to a domain, sites like https://crt.sh and https://ui.ctsearch.entrust.com/ui/ctsearchui offer a searchable database of certificates that shows current and historical results.








## DNS Brute force
dnsrecon








## Virtual Hosts

More than one site on a machine:

- Name based: "host-based routing", or "virtual host routing", 
where the server uses the Host header in the HTTP request to determine which application is meant to 
handle the request.

- IP based: different subdomains will have different IP addresses, so when our system goes to look up the subdomain, it gets the address of the server that handles that application.

Some subdomains aren't always hosted in publically accessible DNS results, such as development versions of a web application or administration portals. Instead, the DNS record could be kept on a private DNS server or recorded on the developer's machines in their /etc/hosts file (or c:\windows\system32\drivers\etc\hosts file for Windows users) which maps domain names to IP addresses.

Because web servers can host multiple websites from one server when a website is requested from a client, the server knows which website the client wants from the Host header. We can utilise this host header by making changes to it and monitoring the response to see if we've discovered a new website.



Using `ffuf`: the -w switch to specify the wordlist we are going to use. The -H switch adds/edits a header (in this instance, the Host header), we have the FUZZ keyword in the space where a subdomain would normally go, and this is where we will try all the options from the wordlist.
```
user@machine$ ffuf -w /usr/share/wordlists/SecLists/Discovery/DNS/namelist.txt -H "Host: FUZZ.acmeitsupport.thm" -u http://10.10.93.140
```

Because the above command will always produce a valid result, we need to filter the output. We can do this by using the page size result with the `-fs` switch. Edit the below command replacing {size} with the most occurring size value from the previous result.

```
user@machine$ ffuf -w /usr/share/wordlists/SecLists/Discovery/DNS/namelist.txt -H "Host: FUZZ.acmeitsupport.thm" -u http://10.10.93.140 -fs {size}
```

This command has a similar syntax to the first apart from the -fs switch, which tells ffuf to ignore any results that are of the specified size.



Using `gobuster`:

```

gobuster vhost -w /opt/useful/SecLists/Discovery/DNS/subdomains-top1million-5000.txt -u http://example.com --append-domain

```












From <https://www.youtube.com/watch?v=QGRwYrUixNc>
```
host -v -t NS [website]

#Zone Transfer
host -l -a

# Reverse DNS
tell how many domains resolve in this IP
dig -x [IP host] @[name server]
for i in {1..255}; do dig -x [IP host] @[name server]; done

# Google Hacking
site:dw.com -www.dw.com
- save result page as html
- then:
	w3m [file.html] > file.txt (the w3m utility parses the html file as seen in Lynx CLI browser)

ejemplo con dw.com
grep -i \.dw.com dw.txt | grep https | cut -d " " -f 1 | sort | unique

# buscar certificados digital <https://crt.sh>
%.dw.com (see more un Advanced menu)
sss

# nmap
sudo nmap -n -Pn --script hhtp-vhosts -p443 www.dw.com

```